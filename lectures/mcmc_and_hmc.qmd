---
title: ""
format: 
  revealjs:
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    theme: simple
    incremental: false
    embed-resources: true
    css: style.css
    
---

##
<center>
<h2>Markov Chain Monte Carlo & Hamiltonian Monte Carlo</h2>
</center>
  
![](./images/17/f_this_mcmc.jpg){width="50%"}


```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)

opts_chunk$set(fig.height=5, fig.width=7, comment=NA, 
               warning=FALSE, message=FALSE, 
               dev="jpeg", echo=FALSE)

library(rethinking)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tidybayes)
library(tidybayes.rethinking)

#center plot titles
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5))


```

## Our Story Thus Far...
- We have been using Maximum A Posteriori Approximations  
  
- Assumes Gaussian posterior (approximately quadratic)  
  
- Great for simple linear Gaussian models

```{r int}
data(rugged)
rugged$log_gdp <- log(rugged$rgdppc_2000)

#prep the data
d <- rugged[complete.cases(rugged$log_gdp),]

#need indices >0
d$cont_africa <- d$cont_africa +1

#The model
mod_int <- alist(
  #likelihood
  log_gdp ~ dnorm(mu, sigma),

  #data generating process
  mu <- bA[cont_africa] + bR[cont_africa]*rugged,

  #priors
  bR[cont_africa] ~ dnorm(0,1),
  bA[cont_africa] ~ dnorm(8,100),
  sigma ~ dunif(0,10))

fit_int <- quap(mod_int, data=d)
samp_int <- tidy_draws(fit_int)
samp_int |>
  rename(`Not Africa` = `bR[1]`, 
         `Africa` = `bR[2]`) |>
  select(.draw:Africa) |>
  tidyr::pivot_longer(-.draw) |>
  ggplot(aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  labs(x = "Standardized Slope", fill = "Continent", y = "Density",
       title = "Slope Posteriors")


#plot(density(extract.samples(fit_no_int)$bR), main="Slope Posterior")
```

## But...
- Problems with models of moderate complexity  
     - GLMs can get odd
  
- Many problems do not have easy analytical solution  
    - Autocorrelation  
    - State-Space Models
    - Mixed Models  
  
- Solution is simulated draws form the posterior...

## Monte Carlo Sampling for Bayesian Estimation

1. Markov Chain Monte Carlo (MCMC)   

2. The problems of MCMC  

3. Hamiltonian Monte-Carlo (HMC)  

4. HMC v. MCMC  

5. Implementing HMC

## King Markov and His Islands
![](./images/17/Mergui-Archipelago.jpg)

## King Markov and His Islands
![](./images/17/Mergui-Archipelago.jpg)
![](./images/17/boat2.jpg)

## How to move around Islands
![](./images/17/boat2.jpg)

<li class="fragment">Flip a coin. Heads, move left. Tails, move right.</li>  
<li class="fragment">Figure out the number of people on each island.</li>  
<li class="fragment">Assign a probability, p = # next island / # current island</li>  
<li class="fragment">Choose a random number. If number < p, move.</li>
<li class="fragment">Rinse and repeat</li>

## What Metropolis MCMC Looks Like
```{r mcmc, echo=TRUE}

mcmc_ex <- function(num_weeks = 1e5, 
                    current=10, 
                    positions = rep(0, num_weeks)){
  for ( i in 1:num_weeks ) {
    # record current position
    positions[i] <- current
  
    # flip coin to generate proposal
    proposal <- current + sample( c(-1,1) , size=1 )
    
    # now make sure he loops around the archipelago
    if ( proposal < 1 ) proposal <- 10
    if ( proposal > 10 ) proposal <- 1
  
    # move?
    prob_move <- proposal/current
    current <- ifelse( runif(1) < prob_move , proposal , current )
  }

  positions
}

```


## Metropolis MCMC in Action: 10 Weeks
Population = Island Number
```{r met}
par(mfrow=c(1,2))
plot(mcmc_ex(10), xlab="weeks", type="l")
simplehist(mcmc_ex(10))
par(mfrow=c(1,1))
```

## Metropolis MCMC in Action: 50 Weeks
Population = Island Number
```{r met_50}
par(mfrow=c(1,2))
plot(mcmc_ex(50), xlab="weeks", type="l")
simplehist(mcmc_ex(50))
par(mfrow=c(1,1))
```

## Metropolis MCMC in Action: 1000 Weeks
Population = Island Number
```{r met_1000}
par(mfrow=c(1,2))
plot(mcmc_ex(1000), xlab="weeks", type="l")
simplehist(mcmc_ex(1000))
par(mfrow=c(1,1))
```

## Metropolis MCMC For Models
::: {layout-ncol=2}

###
```{r tmp}
plot(mcmc_ex(100), xlab="weeks", type="l")
```

###

- Each island is a set of parameter choices  
  
- Each "population" is a posterior density  
  
- The path is a 'chain'  
  
- Note the autocorrelation - we "thin" chains 
    - Only use every ith sample so that there is no autocorrelation
:::

## MCMC In Practice for Models
![](./images/17/mcmc_in_practice.jpg)  
  
  
  
<div style="font-size:11pt">from http://www.cnblogs.com/Nietzsche/p/4255948.html</div>

## Monte Carlo Sampling for Bayesian Estimation

1. Markov Chain Monte Carlo (MCMC)   

2. [The problems of MCMC]{.red}  

3. Hamiltonian Monte-Carlo (HMC)  

4. HMC v. MCMC  

5. Implementing HMC

## MCMC is not a Panacea
![](./images/17/mcmc_fail.jpg)

## MCMC is not a Panacea
![](./images/17/mcmc_stuck_green_worden_2015.jpg)

## How can MCMC Fail?

- MCMC (particularly Metropolis) can get stuck  
     - High correlation between parameters
     - Concentration of Measure problem
  
- Start values can still be important  
     - But, who can always choose the right start values?
  
- One way we try and assess is fit with many chains and make sure they converge
      - This can lead to depression as things never converge, or converge in impossible values
      
## Concentration of Measure Problem

```{r 2d_conc}
D <- 1000
T <- 1e3
rad_dist <- function( Y ) sqrt( sum(Y^2) )

set.seed(2024)
Y <- rmvnorm(T,rep(0,D),diag(D)) |> as_tibble()
ggplot(Y) +
  geom_density2d_filled(data = Y, aes(x = V10, y = V2)) +
  geom_hline(yintercept = 0, color = "red")+
  geom_vline(xintercept = 0, color = "red") +
  labs(subtitle = "bivariate plot of two axis of 1000-D normal distribution\nwith mode at 0,0")
```

When you sample at high dimensions, as more of the density is away from the mode, you sample further and further from the mode


## Concentration of Measure Problem
```{r}
D <- 10
T <- 1e3
rad_dist <- function( Y ) sqrt( sum(Y^2) )
Rd <- sapply( 1:T , function(i) rad_dist( Y[i,] ) )

get_dists <- function(D = 10, T=1e3){
  Y <- rmvnorm(T,rep(0,D),diag(D))
  sapply( 1:T , function(i) rad_dist( Y[i,] ) )
}

cmp_dat <- tibble(D = c(1, 10, 50, 100, 500, 1000)) |>
  group_by(D) |>
  reframe(x = get_dists(D)) 

ggplot(cmp_dat, aes(x=x, fill = factor(D))) +
  geom_density(alpha = 0.7) +
  labs(fill = "Dimension", x = "Radial Distance from Mode of MV Gaussian")
```


## This is a Real Problem - Perhaps Bigger Than We Knew

![](./images/17/padme_mcmc_convergence.jpg)

## MCMC Algorithm Solution
<div id="left">
- Metropolis MCMC inefficient and prone to getting stuck  
  
- Many algorithms to come up with clever proposed moves to speed up  
  
- Gibbs sampling used for BUGS, JAGS, etc.  
    - Still has same problems as Metropolis  
  
- Or... Abandon search and use more deterministic sampling
    - Hamiltonian MCMC
</div>

<div id="right">
  
  
![](./images/17/gibbs_strangelove.jpg)
</div>

## Monte Carlo Sampling for Bayesian Estimation

1. Markov Chain Monte Carlo (MCMC)   

2. The problems of MCMC  

3. [Hamiltonian Monte-Carlo (HMC)]{.red}  

4. HMC v. MCMC  

5. Implementing HMC

## King Monty Of the Valley and Hamilton, His Advisor

![](./images/17/town_in_valley.jpg)


## Moving Around the Valley

![](./images/17/thomas_and_ti_2020.png)

- Royal carraige pushed in random direction with random momentum.    
  
- As it moves up the hill, slows down until it reverses.  
  
- After a time, we stop it and record its location

- Will spend more time in at bottom of the hill (most people)

[arXiv:2006.16194v2]{.footer}

## How Monty Samples
![](./images/17/hmc_trajectory_mcelreath.png)


## A Few Tuning/Algorithmic Details

- We need to chose how large a step we have between each time the gradient is recalculated
     - Automated in the *Warmup phase*
     - Warmpup is NOT burn-in - no information 
     - These are Leapfrog steps  
     
- We do NOT want our chain to come back to where it started  
     - No U Turn Sampling - NUTS  
     - Calibrated during warmup as well  
     - Algorithms always improving
     
## Monte Carlo Sampling for Bayesian Estimation

1. Markov Chain Monte Carlo (MCMC)   

2. The problems of MCMC  

3. Hamiltonian Monte-Carlo (HMC)  

4. [HMC v. MCMC]{.red}  

5. Implementing HMC

## Metropolis versus Hamilton
![](images/17/Prokhorenko_et_al_2018.jpeg)

[Prokhorenko et al 2018]{.footer}

## Metropolis versus Hamiltonian
![](images/17/metropolis_hamilton_bivariate.jpg)
  
  
<div style="font-size:11pt">Neal 2011, http://www.cs.utoronto.ca/~radford/ham-mcmc.abstract.html </div>

## Metropolis versus Hamiltonian
![](images/17/metropolis_hamilton_sampling.jpg)
  
  
  
<div style="font-size:11pt">Neal 2011, http://www.cs.utoronto.ca/~radford/ham-mcmc.abstract.html </div>


## Metropolis versus Hamiltonian
![](images/17/metropolis_hamilton_sampling_high_n.jpg)
  
  
<div style="font-size:11pt">Neal 2011, http://www.cs.utoronto.ca/~radford/ham-mcmc.abstract.html </div>

## Let's Explore HMC and MCMC

- [Chi Feng's HMC/MCMC Explorer](https://chi-feng.github.io/mcmc-demo/app.html)

- [Alex Rogozhnikov's Explainer](https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html)



## Monte Carlo Sampling for Bayesian Estimation

1. Markov Chain Monte Carlo (MCMC)   

2. The problems of MCMC  

3. Hamiltonian Monte-Carlo (HMC)  

4. HMC v. MCMC  

5. [Implementing HMC]{.red}

## Implementing HMCMC via Stan {.smaller}
![](images/17/stan_team.jpg)

- We use the `ulam` function to call STAN  
     - Honors Stanislaw Ulam, one of the originators of Monte-Carlo computing.  
     - Stan is named after him!  
     - Compiles a model, so it can take a while  
  
- Can specify number of chains and other parameters  
  
- And now our samples are already part of our model!  
  
- Careful, models can get large (in size) depending on number of parameters and samples

## Data Prep for HMC
```{r show_rugged, echo=TRUE}
data(rugged)
rugged$log_gdp <- log(rugged$rgdppc_2000)

#Prep the data
d <- rugged[complete.cases(rugged$log_gdp),]

# Need indices >0
d$cont_africa <- d$cont_africa +1

# Only want the data we are using
# (otherwise slows STAN down)
d.trim <- d[ , c("log_gdp","rugged","cont_africa") ]
```

## The Model...
```{r int_model, echo=TRUE}
int_mod <- alist(
  #likelihood
  log_gdp ~ dnorm(mu, sigma),
  
  #Data generating process
  mu <- bR[cont_africa]*rugged + bA[cont_africa],
  
  #priors
  bR[cont_africa] ~ dnorm(0,1),
  bA[cont_africa] ~ dnorm(8,100),
  sigma ~ dcauchy(0,2)
)

```

<div class="fragment">Wait, Cauchy???</div>

## Sidenote: the Cauchy Distribution
- Pronounced Ko-she
- A ratio of two normal distributions  
- Large thick tail
     - Extreme values regularly sampled
- Uses half-cauchy, so, only positive  

```{r cauchy, fig.height=4, fig.width=7}
crossing(scale = c(1,3,5), value = seq(0,10, length.out=200)) %>%
  mutate(dens = dcauchy(value, 0, scale)) %>%
  ggplot() +
  aes(x=value, y=dens, color=factor(scale)) +
  geom_line(linewidth=1.3)
```

## Cauchy v. Exponential
```{r cauchy_exp}
crossing(scale = 1, value = seq(0,5, length.out=200)) |>
  mutate(cauchy = dcauchy(value, 0, scale),
         exponential = dexp(value, scale)) |>
  pivot_longer(cols = cauchy:exponential, values_to = "density", names_to = "distribution") |>
  ggplot() +
  aes(x=value, y=density, color=distribution) +
  geom_line(linewidth=1.3)
```

## Fitting with ulam
```{r map2stan, cache=FALSE, echo=TRUE}
fit <- ulam(int_mod, data=d.trim)
```

- Note where errors occur  
     - Warmup only?  
     - How often in your chain?

## How Much Time Did it Take?
:::{layout-ncol=2}
###
```{r}
#| echo: true
show(fit)
```

###
![](./images/17/mcmc_meme_raiders.png)
:::

## How Do Your Chains Look?

```{r}
#| echo: true
traceplot(fit)
```

- Can use `window` argument to remove initial bit

## How Does Your Chain Look?
```{r}
#| echo: true
traceplot(fit, window = c(50, 1e3))
```

## A Well Shaped Posterior
```{r}
#| echo: true
pairs(fit, depth = 2)
```

## Inspect Your Output and Effective Samples

```{r}
#| echo: true
summary(fit)
```

## To See if Chains are Really Covering Sample Space, Refit with Multiple Chains

```{r map2stan_4, cache=FALSE, echo=TRUE}
fit_4 <- ulam(int_mod, data=d.trim,
              chains = 4, cores = 4)
```

## Your Four Chains

```{r}
#| echo: true
show(fit_4)
```

## Inspect your Chains for convergence!

```{r plot_chain}
traceplot(fit_4, window = c(50, 1e3))
```

## The Trankplot
```{r, fig.height=4, fig.width=7}
#| echo: true
trankplot(fit_4)
```

- Shows histogram of sample ranks.  
- Should be freely mixing. 


## Numerical Assessment of Convergence
```{r chains_precis, echo=TRUE}
summary(fit_4)
```

- `n_eff` is effective number of samples in chain  
     - Should be reasonably large (at least 200)   
  
- `Rhat` is a measure of convergence  
     - Gelman-Rubin diagnostic
     - Should be 1 - even 1.01 is suspect  
       
- Treat as warnings - necessary but not sufficient

## General Workflow
1. Fit one chain to make sure things look OK
     - warmup = 1000, iter=2000 OK  
  
2. Fit multiple chains to ensure convergence  
     - Inspect `n_eff` and `r_hat`
     - Make sure posterior converges and is stationary
     - Tune HMC and model parameters if needed 
  
3. Fit one chain with large everything tuned 
    - Can use more chains if you have the processors

     
## What do bad chains look like?
```{r bad_chain_make, results="hide", cache=FALSE, echo = TRUE}
y <- c(-1,1)
set.seed(11)
m9.2 <- ulam(
    alist(
        y ~ dnorm( mu , sigma ) ,
        mu <- alpha ,
        alpha ~ dnorm( 0 , 1000 ) ,
        sigma ~ dexp( 0.0001 )
    ) , data=list(y=y) , chains=3 )

```

## What do bad chains look like?
```{r}
traceplot(m9.2)
```

## What do bad chains look like?
```{r}
trankplot(m9.2)
```

## What do bad chains look like?
```{r}
summary(m9.2)
```

## The Wandering Chains
```{r wandering, results="hide", cache=FALSE, echo = TRUE}
set.seed(384)
m9.4 <- ulam(
    alist(
        y ~ dnorm( mu , sigma ) ,
        mu <- a1 + a2 ,
        a1 ~ dnorm( 0 , 1000 ),
        a2 ~ dnorm( 0 , 1000 ),
        sigma ~ dexp( 1 )
    ) , data=list(y=y) , chains=3 )
```

## Warnings
```
Warning: 1 of 1500 (0.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.

Warning: 1103 of 1500 (74.0%) transitions hit the maximum treedepth limit of 10.
See https://mc-stan.org/misc/warnings for details.
```

Chains inefficient. Can change tree depth, but...

## Wandering Chains

```{r}
traceplot(m9.4)
```

## Wandering Chains

```{r}
trankplot(m9.4)
```

## Lack of Convergence
::: {layout-ncol=2}

###
- Might be that model has not found good values  

- More likely bad model  
     - Too many parameters  
     - Redundant parameters  
     - Poor fit to data
     - Bad priors  

###     
![](./images/17/mcmc_convergence_no.jpg)
:::


## Taming a Bad Chain with Informative Priors
```{r tame_bad_chain, results="hide", cache=FALSE, echo = TRUE}
set.seed(11)
m9.3 <- ulam(
    alist(
        y ~ dnorm( mu , sigma ) ,
        mu <- alpha ,
        alpha ~ dnorm( 1 , 10 ) ,
        sigma ~ dexp( 1 )
    ) , data=list(y=y) , chains=3 )

precis( m9.3 )
```

## Still Looks Odd...

```{r}
traceplot(m9.3)
```

## Eh....

```{r}
trankplot(m9.3)
```

#

**Folk theorem of statistical computing:** When you are having trouble fitting a model, it often indicates a bad model.

<br><br>

In many ways, HMC and MCMC failing will help you find issues in your own intuition in both model structure and how to build a good model, rather than just being a PITA.

#

![](./images/17/francescacapel_corner_plot.png)

[Frances Capel](https://francescacapel.com/BayesianWorkflow/notebooks/introduction.html){.footer}