---
title: ""
format: 
  revealjs:
    navigation-mode: vertical
    controls-layout: bottom-right
    controls-tutorial: true
    theme: simple
    incremental: false
    css: style.css
    
---

#
<center>
<h2>Information Theory and a Multimodel World</h2>
</center>
  
![](./images/15/army_of_aic.jpg){width="60%"}


```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)

opts_chunk$set(fig.height=5, fig.width=7, comment=NA, 
               warning=FALSE, message=FALSE, 
               dev="jpeg", echo=FALSE)

library(rethinking)
library(tidybayes)
library(tidybayes.rethinking)
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_classic(base_size = 18))
```

# How complex a model do you need to be useful?
![](./images/15/matter-model.jpg)

# Some models are simple but good enough
![](./images/15/turtles.jpg)

# More Complex Models are Not Always Better or Right
![](./images/15/helio_geo.jpg)

# Our Old Friend $R^2$, but Bayesian

$$R^2 = \frac{Var_{model}}{Var_{data}}$$ 
  
[But, with some priors, this can be negative]{.fragment}

[$$R^2_{bayes} = \frac{Var_{model}}{Var_{model} + Var_{residual}}$$]{.fragment} 

[Is this useful?]{.fragment .center .red}

## A Bayesian R2 Function
```{r r2}

r2_bayes_rethinking <- function(fit_mod, variable){
  mu_obs <- sim(fit_mod)
  y <- fit@data[[variable]]
  
  res <- apply(mu_obs, 1, \(x) {x - y})
  
  mu_var <- var(colMeans(mu_obs))
  res_var <- var(rowMeans(res))
  
  mu_var/(mu_var+res_var)
}

```

# Consider this data...
```{r, echo=TRUE}

sppnames <- c( "afarensis","africanus",
               "habilis","boisei",
               "rudolfensis","ergaster","sapiens")

brainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )
masskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )

d <- data.frame( species=sppnames, 
                 brain=brainvolcc, 
                 mass=masskg )
```

# Underfitting
```{r underfit}
baseplot <- ggplot(data = d, mapping=aes(y=brainvolcc, x=masskg)) +
  geom_point(shape=1, size=2) +
  theme_bw(base_size=18)

baseplot +
  stat_smooth(method="lm", formula = "y ~ 1") +
  ggtitle("k=0") 
```

We have explained nothing!

# Overfitting
```{r overfit}
baseplot +
  stat_smooth(method="glm", formula = "y ~ poly(x,6)")  +
  ggtitle("k=6")
```

We have perfectly explained this sample

# What is the right fit? How do weGet there?
```{r medfit}
library(gridExtra)

grid.arrange(
baseplot +
  stat_smooth(method="lm", formula = "y ~ poly(x,1)") +
  ggtitle("k=1"),

baseplot +
  stat_smooth(method="lm", formula = "y ~ poly(x,2)") +
  ggtitle("k=2"), nrow=1 )
```

# {data-background="images/15/scylla_charybdis.jpeg" data-background-size="contain" background-color="black"}

# How do we navigate Scylla and Charybdis?
1. Information theory   
  
2. Regularization  
    - Force parameters to demonstrate they have strong support
    - Penalize parameters with weak support
  
3. Optimization for Prediction  
    - Cross-Validation  
    - Information Theory
    - Drawing from comparison of information loss

# Information Theory and Entropy

```{r}
truth_plot <- ggplot(data = d, mapping=aes(y=brainvolcc, x=masskg)) +
  stat_smooth(method = "lm", formula = y~poly(x,6))

truth_mod <- lm(brainvolcc ~ poly(masskg, 6), data = d)
linear_mod <- lm(brainvolcc ~ masskg, data = d)

truth_plot + labs(subtitle="Assume this is the truth...")

get_dens <- function(x){
  dx <- density(x)
  
  approx(dx$x,dx$y, xout = x)
}

pred_frame <- tibble(
  mass = seq(34, 62, .01),
  pred_6 = predict(truth_mod, newdata = data.frame(masskg = mass)),
  p_6 = get_dens(pred_6)$y,
  pred_1 = predict(linear_mod, newdata = data.frame(masskg = mass)),
  p_1 = get_dens(pred_1)$y
)

```

# Entropy: Over the distribution of brainvolcc, what is the average log probability of observing values

$$H = - E[log(p_i)] = -\sum p_i log(p_i)$$

:::{.incremental}
- sum over the curve

- notice higher entropy the more a wiggly curve as it repeats the same values  
  
- a straight line will have lower entropy from a uniform distribution  
:::


# Difference between the truth and a model

```{r}
ggplot(pred_frame,
       aes(x = mass)) +
  geom_line(aes(y = pred_6), color = "blue", linewidth = 2) +
  geom_line(aes(y = pred_1), color = "orange", linewidth = 2) +
  geom_ribbon(aes(ymin = pred_1, ymax = pred_6), fill = "lightgrey") +
  labs(y = "brainvolcc", x = "masskg")
  
```

# Kullback-Leibler Divergence 

**Divergence:** The additional uncertainty induced by using probabilities from one distribution to describe another distribution.


$$D_{KL} =  \sum p_i (log(p_i) − log(q_i)) \\
= \sum p_i log\frac{p_i}{q_i}$$ 
where $q_i$ is the probability of a value from the model and $p_i$ from the truth. This is the difference in information caused by using the model to approximate the truth.

# But - we want to **compare models**

Difference in information loss between Model 1 (q) and Model 2 (r)

[$$ =  \sum p_i (log(p_i) − log(q_i) - \sum p_i (log(p_i) − log(r_i) \\$$]{.fragment}
[$$= \sum p_i log(p_i) − p_i log(p_i) - p_i log(q_i) + p_i log(r_i)$$]{.fragment}
[$$= \sum p_i log(r_i) - p_i log(q_i)$$]{.fragment}
[$$=\sum p_i(log(r_i) - log(q_i))$$]{.fragment} 
[$$= E[log (r_i)] - E[log(q_i)]$$]{.fragment}


# So, What is Our Score of Interest Then?
It's all about $E[log(q_i)]$!

$$S(q) = \sum log(q_i)$$

:::{.incremental}

- This is the unscaled average of our model score.  
  
- It is also the *log pointwise predictive density*.  
  
- Provides an estimate of how well our model's predictive accuracy.  

- Pointwise, and uses entire posterior for each observation. 
:::

# lppd for one point, visually
:::{.columns}

:::{.column width=50%}
```{r}
d <- d |>
  mutate(brain_s = standardize(brain),
         mass_s = standardize(mass))

mod <- alist(
  brain_s ~ dnorm(mu, sigma),
  mu <- a + b*mass_s,
  a ~ dnorm(0,1),
  b ~ dnorm(0,1),
  sigma ~ dexp(2)

)

fit <- quap(mod, data = d,
            method= "Nelder-Mead")

pred_fit <- linpred_draws(fit, d, ndraws = 1e3)

ggplot(d, aes(x = mass_s, y = brain_s)) +
  stat_lineribbon(data = pred_fit, aes(y = .value)) +
  geom_point() +
  scale_fill_brewer() +
  geom_point(x = d$mass_s[5], y = d$brain_s[5], color = "red",
             size = 8, pch = 1) 

```

:::

:::{.column width=50%}
```{r}
point_5 <- linpred_draws(fit, newdata = data.frame(mass_s = d$mass_s[5]))

ggplot(point_5, aes(x = .value)) +
  geom_density(linewidth = 2) +
  labs(x = "posterior brain value given std mass = 0.917")+
  labs(title = paste0("lppd = ", round(lppd(fit)[5], 3)), y = "p(y|theta)")
```

:::

:::


# lppd and Deviance

$$ Deviance \approx -2 *lppd$$

- Hello, old friend.  
  
- Increasing model complexity will **always** improve lppd.  
  
- Out of sample lppd will get worse with model complexity. 


# In and Out of Sample Deviance
```{r dev_plot1}
linplot <- baseplot +
  stat_smooth(method="lm", formula = "y ~ poly(x,1)") 

linplot

lmreg <- lm(brainvolcc ~ masskg, data=d)
```
<div class="fragment">Deviance = -2 * log(dnorm($y_i | \mu_i$))  </div>
<div class="fragment">The deviance of this linear regression is `r deviance(lmreg)`</div>

# In and Out of Sample Deviance
```{r dev_plot2}
linplot +
  geom_point(size=2, x=50, y=515, color="red")
```

Prediction: `r predict(lmreg, newdata=data.frame(masskg=50))`, Observe: 515  
Deviance: `r -2*dnorm(807, 515, log=TRUE)`

# In and Out of Sample Deviance
```{r simtrain, cache=TRUE, results="hide"}
stt <- function(n=1e3, k=1, b_sigma=100, ...){
 # print(k)
  set.seed(2002)
  ret <- mcreplicate(n, sim.train.test(k=k, ...), mc.cores=4)
  ret <- as.data.frame(ret)
  ret$type <- c("Train", "Test")
  ret %>% gather(v, dev, -type)
}

simdf <- data.frame(r=1:5) %>%
  group_by(k=1:5) %>%
  nest() %>%
  mutate(var = purrr::map(data, ~stt(k=.$r[1]))) %>%
  unnest(var) %>%
  group_by(type, k) %>%
  summarize(mean_deviance = mean(dev),
            deviance_lwr = mean_deviance-sd(dev),
            deviance_upr = mean_deviance+sd(dev)
  ) %>%
  ungroup()
```

```{r simtrainplot}
base_simplot <- ggplot(simdf, mapping=aes(x=k, y=mean_deviance, 
                          ymin=deviance_lwr, ymax=deviance_upr,
                          color=type, shape=type)) +
  geom_point(size=2) +
  geom_line() +
  theme_bw(base_size=17)+
  scale_shape_manual(values=c(1,2,3)) +
  scale_color_manual(values=c("black", "blue"))


base_simplot
```

# Our Goal for Judging Models
- Can we minimize the out of sample deviance  
  
- So, fit a model, and evaluate how different the deviance is for a training versus test data set is  
  
- What can we use to minimize the difference?

# How do we navigate Scylla and Charybdis?
1. Information theory   
  
2. [Regularization]{.red}  
    - Force parameters to demonstrate they have strong support
    - Penalize parameters with weak support
  
3. Optimization for Prediction  
    - Cross-Validation  
    - Information Theory
    - Drawing from comparison of information loss


# Regularlization
<div id="left">
```{r priors, fig.height=6, fig.width=6}
dens_df <- crossing(sigma = c(1,0.2,0.5), val = seq(-3,3, length.out=200)) %>%
  mutate(dens = dnorm(val, 0, sigma))

ggplot(dens_df,
       aes(x = val, y = dens, color = factor(sigma))) +
  geom_line(linewidth = 2) +
  labs(y = "density", x = "", color = "sigma")
  theme_bw(base_size=17)
```

</div>
<div id="right">
  
- Regularization means shrinking the prior towards 0  
  
- Means data has to work harder to overcome prior  
  
- Good way to shrink weak effects with little data, which are often spurious  
  </div>


# Regularization and Train-Test Deviance
```{r regular, results="hide", cache=TRUE}
stt_means <- function(n=1e2, k=1, b_sigma, ...){
  ret <- mcreplicate(n, {sim_train_test(k=k, b_sigma = b_sigma, ...)}, mc.cores=7)
  rowMeans(ret) |> as_tibble() |>
    mutate(
      sd = c(sd(ret[1,]), sd(ret[2,])),
      type = c("Train", "Test"),
      k = k, sigma = b_sigma) |>
    rename(mean_deviance = value)
}


regulardf <- crossing(k=1:5, b_sigma=c(4, 1, 0.2)) |>
  group_by(a=1:n()) |>
  reframe(stt_means(n = 1e3, k = k, b_sigma = b_sigma, N = 20))

```

```{r regularplot}

ggplot(data = regulardf,
       aes(x = k, y = mean_deviance, shape = factor(sigma), 
           linetype = factor(type), color = factor(sigma))) +
  geom_point(size=4) +
  geom_line() +
  theme_bw(base_size=17) +
  scale_color_brewer(palette = "Set1")+
  #scale_shape_manual(values=c(1,2,3)) +
  #scale_color_manual(values=c("black", "blue")) +
  labs(shape = "sigma", color="sigma", shape = "type")


```


# How do we navigate Scylla and Charybdis?
1. Information theory   
  
2. Regularization 
    - Force parameters to demonstrate they have strong support
    - Penalize parameters with weak support
  
3. [Optimization for Prediction]{.red}   
    - Cross-Validation  
    - Information Theory
    - Drawing from comparison of information loss


# A Criteria Estimating Test Sample Deviance
- What if we could estimate out of sample deviance?  
  
- The difference between training and testing deviance shows overfitting  
  

# A Criteria Estimating Test Sample Deviance
```{r diff}
diff_df <- regulardf %>% 
  select(-sd) %>%
  pivot_wider(names_from = type,  values_from = mean_deviance) |>
  mutate(difference = Test - Train)

ggplot(diff_df, aes(x = k, y = difference)) +
  geom_point() +
  stat_smooth(method="lm")
```
Slope here of `r round(coef(lm(difference ~ k, data=diff_df))[2], 2)`

# AIC
<div id="right">
![](./images/15/hirotugu_akaike.jpg)
</div>
<div id="left">
- So, $E[D_{test}] = D_{train} + 2K$  
  
- This is Akaike's Information Criteria (AIC)  
  
</div>

$$AIC = Deviance + 2K$$


# AIC
- Estimate of Leave One Out Cross-Validation   

- AIC optimized for forecasting (out of sample deviance)  
  
- Requires flat priors  
  
- Assumes large N relative to K  
    - AICc for a correction  
  
- Difficult to define in a mixed model context

# What about IC in Bayes?
- We *do* estimate the posterior distribution of the deviance  
  
- Average of the posterior, $\bar{D}$ is our $D_{train}$  
  
- But what about # of parameters?  
    - For a non-mixed model, this is not a concern - just the AIC  
    - For a mixed model...trickier

# Effective number of Parameters
- In addition to $\bar{D}$, there is also $\hat{D}$  
    - The value of the posterior at the posterior mean  
  
- Given Bayesian logic: 
    - $\bar{D}$ is our estimate of the out of sample deviance
    - $\hat{D}$ is our $D_{train}$ 
    - So, $\bar{D} - \hat{D}$ = number of parameters  
  
- We term this $p_D$ for effective number of parameters
  
# DIC

![](./images/15/Sir-David-Spiegelhalter_I0A0548.jpg)
  
$$DIC = 2 \bar{D} - 2 p_D$$  


# DIC

$$DIC = 2 \bar{D} - 2 p_D$$  

- Works well for multivariate normal posteriors  
- Handles mixed models  
- Reduces to AIC with flat priors  
- But does not require flat priors - which does interesting things to $p_D$!
- But... fails with anything non-normal, and hot debate on even mixed effects

# And so we pause...
- Our goal is to maximize prediction  
  
- Why not look at the pieces that make up the deviance  
    - The pointwise predictive power of the posterior  
  
- We can define the Pr(y<sub>i</sub> | posterior simulations)  
  - This tells us the distribution of the predictive power of our posterior for each point  
  - $lppd = \sum log Pr(y_i | \theta)$
  
# But what about Parameters?
- We know that as k increases, our uncertainty in coefficients increases  
  
- As uncertainty increases, Pr(y<sub>i</sub> | simulations) widens  
  
- Thus, this variance gives us an effective penalty term  
  
- $p_{waic} = \sum Var(log Pr(y_i | \theta))$

# Widely Applicable IC
$$WAIC = -2 \sum log Pr(y_i | \theta) + 2 \sum Var(log Pr(y_i | \theta))$$  
  
$$= -2 lppd + 2 p_{waic}$$
  

- Widely Applicable/Wantanabe-Akaike Information Criteria  
  
- Advantage in being pointwise is that we also get an estimate of uncertainty  
  
- Disadvantage that inappropriate to use with lagged (spatial or temporal) predictors

# But What about Leave One Out?

The problem

$$LOOCV_{Bayes} = \\lppd_{cv} = \\ \sum_{i=1}^N \frac{1}{S} \sum_{s=1}^S log(y_i | \theta_{s, -i})$$
Here, i is observation, 1....N and s is draw from our posterior 1....S. 

In essence, for each data point:

:::{.incremental}
- Remove it & refit the model.  
- Calculate the lppd for that refit.  
- Sum it over each refit.  
- Let your computer warm your house.  
:::

# Don't wait to weight!

:::{.incremental}
- We can do the same if we weighted the lppd of each point by it's importance.  
  
- Importance can be defined as $\frac{p(y_i | \theta_{s, -i})}{p(y_i | \theta_{s})}$

- Proportional to just $\frac{1}{p(y_i | \theta_{s})} = r_s$

- Can weight each element of lppd and then standardize by sum of weights.  

- BUT - if we get weird weights, this can cause huge problems
:::

# Introducing, the Pareto Distribution

```{r pareto}
tibble(x = seq(1,5, .01),
       d = EnvStats::dpareto(x, 1, 1)) |>
  ggplot(aes(x=x, y=d)) + 
  geom_line() + 
  labs(x="", y = "density", subtitle = "pareto, shape = 1, location = 1")
```

- Distribution of importances
      - 80:20 rule

- For each observation, we can use the largest importance value, and calculate a smoothed Pareto curve of weights

- PSIS: Pareto Smoothed Importance Sampling

# Which should I use?
- They are all pretty darned similar!    
  
- WAIC performs better at lower sample sizes. 
  
- WAIC also can perform better in some nonlinear cases.  

- But, PSIS will also give you information about points driving performance and problems.  

# How do I use my IC?
We can use difference between fits or calculate:  
  
$$w_{i} = \frac{e^{\Delta_{i}/2 }}{\displaystyle \sum^R_{r=1} e^{\Delta_{i}/2 }}$$  
Where $w_{i}$ is the *relative support* for model i making the best prediction
compared to other models in the set being considered.  
  
Model weights summed together = 1

# Monkies and Milk
![](./images/14/monkies_milk.jpg)

```{r monkey_load, echo=TRUE}
data(milk)
d <- milk[ complete.cases(milk) , ]
d$neocortex <- d$neocortex.perc / 100
```

# A lotta Models
```{r mods, echo=TRUE}
a.start <- mean(d$kcal.per.g)
sigma.start <- log(sd(d$kcal.per.g))

#null
m6.11 <- map(
    alist(
        kcal.per.g ~ dnorm( a , exp(log.sigma) )
    ) ,
    data=d , start=list(a=a.start,log.sigma=sigma.start) )

#neocortex only
m6.12 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , exp(log.sigma) ) ,
        mu <- a + bn*neocortex
    ) ,
    data=d , start=list(a=a.start,bn=0,log.sigma=sigma.start) )
```

# A lotta Models
```{r moar_mods, echo=TRUE}
# log(mass) only
m6.13 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , exp(log.sigma) ) ,
        mu <- a + bm*log(mass)
    ) ,
    data=d , start=list(a=a.start,bm=0,log.sigma=sigma.start) )

# neocortex + log(mass)
m6.14 <- map(
    alist(
        kcal.per.g ~ dnorm( mu , exp(log.sigma) ) ,
        mu <- a + bn*neocortex + bm*log(mass)
    ) ,
    data=d , start=list(a=a.start,bn=0,bm=0,log.sigma=sigma.start) )
```

# A WAIC
```{r waic, echo=TRUE, refresh=0}
WAIC( m6.14 )
```

# Comparing Models
```{r waic_compare, echo=TRUE}
milk.models <- compare( m6.11 , m6.12 , m6.13 , m6.14)
milk.models
```

# Comparing Models
```{r waic_compare_fig, echo=TRUE}
plot(milk.models, cex=1.5)
```

<Center>solid = deviance, circle=WAIC, triangle = $\Delta$WAIC</center>


# Comparing Models with PSIS
```{r psis_compare, echo=TRUE}
milk.models.psis <- compare( m6.11 , m6.12 , m6.13 , m6.14, func = PSIS)
plot(milk.models.psis)
```

# PSIS and Warnings of Big Weights

```{r psis_warn, warning=TRUE}
PSIS(m6.11)
```

# PSIS and Warnings of Big Weights

```{r psis_warn1, warning=TRUE}
PSIS(m6.11, pointwise = TRUE)
```

# Using WAIC and PSIS to Determine Problem Points
```{r psis_waic_importance}
set.seed(2023)
p <- PSIS(m6.11, pointwise = TRUE)
w <- WAIC(m6.11, pointwise = TRUE)

problem_df <- tibble(`PSIS k` = p$k, `WAIC penalty` = w$penalty, 
                     species = d$species) 

problem_df|>
  ggplot(aes(x = `PSIS k`, y = `WAIC penalty`)) +
  geom_point() +
  geom_vline(xintercept = 0.5, lty = 2)

problem_df |> filter(`PSIS k`>0.5) |> pull(species) |> as.character()
```

# {data-background-color="black"}
  
  
![](./images/15/chorus_line_model_selection.jpg)

# Death to model selection
- While sometimes the model you should use is clear, more often it is *not*  
  
- Further, you made those models for a reason: you suspect those terms are important  
  
- Better to look at coefficients across models  
  
- For actual predictions, ensemble predictions provide real uncertainty

# {data-background-color="black"}
  
![](./images/15/bjork-on-phone-yes-i-am-all-about-the-deviance-let-us-make-it-shrink-our-parameters.jpg)

# Coefficients
Remember, `m6.14` has a 97% WAIC model weight  
  
```{r coeftab, echo=TRUE}
ctab <- coeftab( m6.11 , m6.12 , m6.13 , m6.14)
ctab
```

# Coefficients
Remember, `m6.14` has a 97% WAIC model weight  
  
```{r coeftab_plot, echo=TRUE}
plot(ctab)
```

# Ensemble Prediction
- Ensemble prediction gives us better uncertainty estimates  
  
- Takes relative weights of predictions into account  
  
- Takes weights of coefficients into account  
  
- Basicaly, get simulated predicted values, multiply them by model weight

# Making an Ensemble
```{r base_predict, results="hide", cache=TRUE}
nc.seq <- seq(from=0.5,to=0.8,length.out=30)
d.predict <- data.frame(
    kcal.per.g = rep(0,30), # empty outcome
    neocortex = nc.seq,     # sequence of neocortex
    mass = rep(4.5,30)      # average mass
)

pred.m6.14 <- link( m6.14 , data=d.predict )
mu <- apply( pred.m6.14 , 2 , mean )
mu.PI <- apply( pred.m6.14 , 2 , PI )
```

```{r ensemble, echo=TRUE, results="hide", cache=TRUE}
# data for prediction
nc.seq <- seq(from=0.5,to=0.8,length.out=30)

d.predict <- data.frame(
    kcal.per.g = rep(0,30), # empty outcome
    neocortex = nc.seq,     # sequence of neocortex
    mass = rep(4.5,30)      # average mass
)

# make the ensemble
milk.ensemble <- ensemble( m6.11, m6.12, 
                           m6.13 ,m6.14 , data=d.predict )

# get 
mu_ensemble <- apply( milk.ensemble$link , 2 , mean )
mu.PI_fit <- apply( milk.ensemble$link , 2 , PI )
```

# Making an Ensemble
```{r plot_ensemble}
d_plot <- rbind(d.predict, d.predict) %>%
  mutate(kcal.per.g = c(mu, mu_ensemble),
         lwr = c(mu.PI[1,], mu.PI_fit[1,]),
         upr = c(mu.PI[2,], mu.PI_fit[2,]),
         type=c(rep("best fit", length(mu)), rep("ensemble", length(mu_ensemble))))

ggplot(d_plot, mapping=aes(x=neocortex, y=kcal.per.g, 
                           ymin=lwr, ymax=upr, 
                           color=type, fill=type, lty=type, size=type)) +
    geom_ribbon(alpha=0.3, lty=1, color=NA) +
  geom_line() +
  scale_fill_manual(values=c("red", "blue"))+
  scale_color_manual(values=c("red", "blue")) +
  scale_linetype_manual(values=c(3,1)) +
  scale_size_manual(values=c(1, 1.5)) +
  theme_bw(base_size=17)

```

# Exercise
- Take your milk multiple predictor models with clade, milk components, both, and neither  
  
- Compare via WAIC  
  
- Get ensemble predictions for each clade